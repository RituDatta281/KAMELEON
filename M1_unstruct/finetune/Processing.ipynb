{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda3f779",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b96b3f2-4ed8-4374-bbf3-f00765c479f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "dataset = \"mimic3\"\n",
    "task = \"mortality\"\n",
    "output_path = \"llm_finetune_data_multitask\"\n",
    "# mimic3_mortality_train_0_notes_checkpoint,\n",
    "\n",
    "        \n",
    "# ==========================================\n",
    "# train data\n",
    "ori_path = f\"llm_finetune_data_ulti/{dataset}_{task}_train_0_notes_checkpoint.jsonl\"\n",
    "\n",
    "with open(ori_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "t = 0\n",
    "\n",
    "    \n",
    "instruction_prev_prev = \"\\nGiven the following task description, patient EHR context, similar patients, and retrieved medical knowledge, Please provide a step-by-step reasoning process that leads to the prediction outcome based on the patient's context and relevant medical knowledge.\\nAfter the reasoning process, provide the prediction label (0/1).\"\n",
    "instruction_prev = \"\\nGiven the following task description, patient EHR context, similar patients, and retrieved medical knowledge...\"\n",
    "instruction_new_reason = \"\\n[Reasoning] Given the following task description, patient EHR context, similar patients, and retrieved medical knowledge, Please provide a step-by-step reasoning process that leads to the prediction outcome based on the patient's context and relevant medical knowledge.\\nAfter the reasoning process, provide the prediction label (0/1).\"\n",
    "instruction_new_pred = \"\\n[Label Prediction] Given the following task description, patient EHR context, similar patients, and retrieved medical knowledge, Please directly predict the label (0/1).\\n\"\n",
    "\n",
    "\n",
    "label_pred_data = []\n",
    "reasoning_data = []\n",
    "patient_id_pattern = re.compile(\n",
    "    r\"# Patient EHR Context #.*?Patient ID:\\s*([^\\s]+)\",\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "context_path = f\"patient_context/base_context/patient_contexts_{dataset}_{task}_notes.json\"\n",
    "patient_data_path = f\"ehr_prepare/pateint_{dataset}_{task}_physician_summary.json\"\n",
    "patient_data = json.load(open(patient_data_path))\n",
    "\n",
    "for item in data:\n",
    "    input_new = item[\"input\"].replace(instruction_prev, instruction_new_reason)\n",
    "        \n",
    "    match = patient_id_pattern.search(input_new)\n",
    "    if match:\n",
    "        patient_id = match.group(1)\n",
    "        t+= 1\n",
    "    else:\n",
    "        print('couldnt find')\n",
    "\n",
    "    output_new = \"# Prediction # \" + str(patient_data[patient_id]['label'])\n",
    "    # print(output_new)\n",
    "    label_pred_data.append({\"input\": input_new, \"output\": output_new})\n",
    "    \n",
    "    # input_new = item[\"input\"].replace(instruction_prev, instruction_new_reason)\n",
    "    output_new = item[\"output\"]\n",
    "    reasoning_data.append({\"input\": input_new, \"output\": output_new})\n",
    "    # break\n",
    "    \n",
    "    \n",
    "data = reasoning_data\n",
    "with open(f\"{output_path}/{dataset}_{task}_train_notes.jsonl\", \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400acab-73a2-4fe6-90bc-1e685ff70334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reasoning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cf3099-0fef-4651-acb6-261994f46556",
   "metadata": {},
   "outputs": [],
   "source": [
    " # test data\n",
    "\n",
    "ori_path = f\"llm_finetune_data_ulti/{dataset}_{task}_test_0_notes_checkpoint.jsonl\"\n",
    "\n",
    "\n",
    "with open(ori_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    \n",
    "    \n",
    "label_pred_data = []\n",
    "\n",
    "for item in data:\n",
    "    input_new = item[\"input\"].replace(instruction_prev, instruction_new_pred)\n",
    "    # output_new = item[\"output\"][-1]\n",
    "    match = patient_id_pattern.search(input_new)\n",
    "    if match:\n",
    "        patient_id = match.group(1)\n",
    "        # print(patient_id)\n",
    "        t+= 1\n",
    "    else:\n",
    "        print('couldnt find')\n",
    "\n",
    "    output_new = \"# Prediction # \" +str(patient_data[patient_id]['label'])\n",
    "    label_pred_data.append({\"input\": input_new, \"output\":output_new})\n",
    "    \n",
    "with open(f\"{output_path}/{dataset}_{task}_test_notes.jsonl\", \"w\") as f:\n",
    "    for item in label_pred_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b201d29c-c86c-48e8-86d0-c55bbc09aeb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"../llm_finetune_data_multitask/mimic3_readmission30_test_notes.jsonl\"\n",
    "output_path = \"alpaca_readmission30_notes_test.json\"\n",
    "\n",
    "alpaca_data = []\n",
    "\n",
    "with open(input_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        output_raw = data[\"output\"]\n",
    "        # if \"# Prediction #\" in output_raw:\n",
    "        #     prediction_part = output_raw.split(\"# Prediction #\")[-1].strip()\n",
    "        # else:\n",
    "        prediction_part = output_raw.strip().replace(\"# Reasoning #\\n\\n\", \"\")  # fallback if missing tag\n",
    "        if \"# Prediction #\" in prediction_part:\n",
    "            output = prediction_part.split(\"# Prediction #\")[-1].strip()\n",
    "        else:\n",
    "            output = prediction_part\n",
    "            \n",
    "        if \"# Reasoning #\" in prediction_part:\n",
    "            reasoning = prediction_part.split(\"# Reasoning #\")[1].strip()\n",
    "        else:\n",
    "            reasoning = \"\"\n",
    "            \n",
    "        alpaca_entry = {\n",
    "            \"instruction\": \"Predict hospital readmission within 30 days based on EHR context.\",\n",
    "            \"input\": data[\"input\"],\n",
    "            \"label\": output,\n",
    "            \"Reasoning\":reasoning\n",
    "        }\n",
    "        alpaca_data.append(alpaca_entry)\n",
    "        # if len(alpaca_data) == 2:\n",
    "        #     break\n",
    "\n",
    "with open(output_path, \"w\") as outfile:\n",
    "    json.dump(alpaca_data, outfile, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a8367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad0015a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd898788-8356-41e0-a440-43f515b73b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    Tesla V100-SXM2-32GB. Num GPUs = 1. Max memory: 31.733 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: unsloth/llama-3-8b-bnb-4bit can only handle sequence lengths of at most 8192.\n",
      "But with kaiokendev's RoPE scaling of 2.0, it can be magically be extended to 16384!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "import json\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "# === SETTINGS ===\n",
    "max_seq_length = 16384\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "lora_path = \"lora_model_readmission30_notes\"\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. Give label 0 if the patient will live, or 1 if he will die in hospital. Then give the reason behind it in 2 lines\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### label:\n",
    "{}\n",
    "\n",
    "### Reasoning:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "# === LOAD DATA ===\n",
    "with open('alpaca_readmission30_notes_test.json', 'r') as jsonfile:\n",
    "    admission = json.load(jsonfile)\n",
    "\n",
    "# === LOAD BASE MODEL ===\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# === LOAD LoRA ADAPTER ===\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    model_id=lora_path,\n",
    "    adapter_name=\"default\"\n",
    ")\n",
    "\n",
    "# === PREPARE FOR INFERENCE ===\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "# === INIT CSV ===\n",
    "csv_file = \"predictions_vs_ground_truth_test_readmission30_notes.csv\"\n",
    "fieldnames = [\"patient_id\", \"ground_truth\", \"prediction\", \"match\", \"reasoning\"]\n",
    "\n",
    "# # # Write header first\n",
    "with open(csv_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53993314-436b-4c8e-a566-857a55c25a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "results = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, entry in enumerate(admission[:]):\n",
    "    input_text = entry[\"input\"]\n",
    "    GT = entry[\"label\"]\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = alpaca_prompt.format(entry[\"instruction\"], input_text, \"\", \"\")\n",
    "    \n",
    "    # Extract patient ID (if available)\n",
    "    match = re.search(r\"# Patient EHR Context #\\n\\nPatient ID:\\s*([^\\s\\n]+)\", input_text)\n",
    "    patient_id = match.group(1) if match else \"UNKNOWN\"\n",
    "\n",
    "    # Generate output\n",
    "    try:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] Skipped due to generation error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Try to extract reasoning section\n",
    "    reasoning_section = prediction.split(\"### Reasoning:\")[-1].strip()\n",
    "    \n",
    "    label_match = re.search(r'# Prediction #\\s*\\n\\s*(\\d)', reasoning_section)\n",
    "\n",
    "    if label_match:\n",
    "        pred_label = label_match.group(1)\n",
    "        try:\n",
    "            y_true.append(int(GT))\n",
    "            y_pred.append(int(pred_label))\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] Label casting failed: {e}\")\n",
    "            continue\n",
    "    # Attempt to extract label\n",
    "    # label_match = re.search(r\"(?:label\\s*[: is]*\\s*)([01])\", prediction.lower())\n",
    "    # if label_match:\n",
    "    #     pred_label = label_match.group(1)\n",
    "    #     try:\n",
    "    #         y_true.append(int(GT))\n",
    "    #         y_pred.append(int(pred_label))\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"[{i}] Label casting failed: {e}\")\n",
    "    #         continue\n",
    "    else:\n",
    "        pred_label = \"0\"\n",
    "        print(f\"[{i}] Could not parse label. Saved for manual review.\")\n",
    "            \n",
    "    result = {\n",
    "        \"patient_id\": patient_id,\n",
    "        \"ground_truth\": GT,\n",
    "        \"prediction\": pred_label,\n",
    "        \"match\": str(GT) == str(pred_label),\n",
    "        \"reasoning\": reasoning_section\n",
    "    }\n",
    "    print(result)\n",
    "    results.append(result)\n",
    "\n",
    "    # Save every 10 entries\n",
    "    if (i + 1) % 3 == 0 or (i + 1) == len(admission):\n",
    "        with open(csv_file, \"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result.keys())\n",
    "            writer.writerows(results)\n",
    "        results = []\n",
    "        # break\n",
    "\n",
    "# === METRICS (Skip entries with unparsable labels) ===\n",
    "valid_indices = [i for i, pred in enumerate(y_pred) if pred in [0, 1]]\n",
    "y_true_valid = [y_true[i] for i in valid_indices]\n",
    "y_pred_valid = [y_pred[i] for i in valid_indices]\n",
    "\n",
    "if y_true_valid:\n",
    "    accuracy = accuracy_score(y_true_valid, y_pred_valid)\n",
    "    precision = precision_score(y_true_valid, y_pred_valid, zero_division=0)\n",
    "    recall = recall_score(y_true_valid, y_pred_valid, zero_division=0)\n",
    "    f1 = f1_score(y_true_valid, y_pred_valid, zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true_valid, y_pred_valid)\n",
    "    except ValueError:\n",
    "        roc_auc = \"N/A (only one class present)\"\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC AUC:   {roc_auc}\")\n",
    "else:\n",
    "    print(\"No valid predictions to compute metrics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311f411-5d1a-4683-9552-08c05d7e0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def parse_output(text):\n",
    "#     prediction_label = None\n",
    "#     prediction_reasoning = None\n",
    "\n",
    "#     # Extract Prediction Label\n",
    "#     match_label = re.search(r\"#### Prediction Label:\\s*(\\d+)\", text)\n",
    "#     if match_label:\n",
    "#         prediction_label = int(match_label.group(1))\n",
    "\n",
    "#     # Extract Prediction Reasoning\n",
    "#     match_reasoning = re.search(r\"#### Prediction Reasoning:\\s*(.*)\", text, re.DOTALL)\n",
    "#     if match_reasoning:\n",
    "#         reasoning = match_reasoning.group(1).strip()\n",
    "#         # If there's any section after Reasoning, clip it\n",
    "#         end_match = re.search(r\"#### \", reasoning)\n",
    "#         if end_match:\n",
    "#             reasoning = reasoning[:end_match.start()].strip()\n",
    "#         prediction_reasoning = reasoning\n",
    "\n",
    "#     return {\n",
    "#         \"label\": prediction_label,\n",
    "#         \"reasoning\": prediction_reasoning\n",
    "#     }\n",
    "\n",
    "# # Example usage\n",
    "# # print(parsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb6fde",
   "metadata": {},
   "source": [
    "## Performance Analaysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d93d8e2-e15c-459f-bad3-465036e72ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6140380407338832,\n",
       " 'confusion_matrix': array([[3357, 1879],\n",
       "        [ 414,  291]]),\n",
       " 'classification_report': {'0': {'precision': 0.8902147971360382,\n",
       "   'recall': 0.6411382734912147,\n",
       "   'f1-score': 0.7454202287110026,\n",
       "   'support': 5236.0},\n",
       "  '1': {'precision': 0.13410138248847928,\n",
       "   'recall': 0.4127659574468085,\n",
       "   'f1-score': 0.20243478260869566,\n",
       "   'support': 705.0},\n",
       "  'accuracy': 0.6140380407338832,\n",
       "  'macro avg': {'precision': 0.5121580898122587,\n",
       "   'recall': 0.5269521154690116,\n",
       "   'f1-score': 0.4739275056598491,\n",
       "   'support': 5941.0},\n",
       "  'weighted avg': {'precision': 0.8004891689040016,\n",
       "   'recall': 0.6140380407338832,\n",
       "   'f1-score': 0.6809858339117891,\n",
       "   'support': 5941.0}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "df_train_LLM = pd.read_csv('predictions_vs_ground_truth_train_mortality_notes.csv')\n",
    "y_true = df_train_LLM[\"ground_truth\"]\n",
    "y_pred = df_train_LLM[\"prediction\"]\n",
    "reasoning = df_train_LLM['reasoning']\n",
    "\n",
    "# # Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "class_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "# Prepare report\n",
    "report = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"confusion_matrix\": conf_matrix,\n",
    "    \"classification_report\": class_report\n",
    "}\n",
    "\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3efa42a-43ef-4ae5-9189-a1ff88f38413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5270\n",
      "AUC-PR: 0.1250\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "# print(classification_report(y_true, y_prd))\n",
    "\n",
    "# Compute AUC-ROC (requires probability scores for positive class)\n",
    "auc_roc = roc_auc_score(y_true, y_pred)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "# Compute AUC-PR (also known as Average Precision Score)\n",
    "auc_pr = average_precision_score(y_true, y_pred)\n",
    "print(f\"AUC-PR: {auc_pr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python unsloth",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
